{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e6bf9f",
   "metadata": {},
   "source": [
    "## BBC News Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7beb08a",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/learn-ai-bbc/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b6293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# nltk imports\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize  # tokenize the text == the text is splitted into words in list\n",
    "from nltk.corpus import stopwords  # this contain common stop words that has no effect in analysis\n",
    "from nltk.stem import WordNetLemmatizer  # Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # bags of words and TF IDF\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, make_scorer  # classification Metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # splitting dataset\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d47d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping , ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef521a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\BBC News\n"
     ]
    }
   ],
   "source": [
    "cd E:\\BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f666f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_Data = pd.read_csv('BBC News Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6dffb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1582</td>\n",
       "      <td>howard  truanted to play snooker  conservative...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>651</td>\n",
       "      <td>wales silent on grand slam talk rhys williams ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1797</td>\n",
       "      <td>french honour for director parker british film...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2034</td>\n",
       "      <td>car giant hit by mercedes slump a slump in pro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1866</td>\n",
       "      <td>fockers fuel festive film chart comedy meet th...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text       Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...       business\n",
       "1        154  german business confidence slides german busin...       business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...       business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...           tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...       business\n",
       "5       1582  howard  truanted to play snooker  conservative...       politics\n",
       "6        651  wales silent on grand slam talk rhys williams ...          sport\n",
       "7       1797  french honour for director parker british film...  entertainment\n",
       "8       2034  car giant hit by mercedes slump a slump in pro...       business\n",
       "9       1866  fockers fuel festive film chart comedy meet th...  entertainment"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBC_Data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f1b47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            346\n",
       "business         336\n",
       "politics         274\n",
       "entertainment    273\n",
       "tech             261\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBC_Data.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed2e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicto = {'sport': 1,'business': 2, 'politics': 3, 'entertainment': 4, 'tech': 5}\n",
    "\n",
    "BBC_Data.Category = BBC_Data.Category.map(dicto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3105835b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1582</td>\n",
       "      <td>howard  truanted to play snooker  conservative...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>651</td>\n",
       "      <td>wales silent on grand slam talk rhys williams ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1797</td>\n",
       "      <td>french honour for director parker british film...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2034</td>\n",
       "      <td>car giant hit by mercedes slump a slump in pro...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1866</td>\n",
       "      <td>fockers fuel festive film chart comedy meet th...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...         2\n",
       "1        154  german business confidence slides german busin...         2\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...         2\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...         5\n",
       "4        917  enron bosses in $168m payout eighteen former e...         2\n",
       "5       1582  howard  truanted to play snooker  conservative...         3\n",
       "6        651  wales silent on grand slam talk rhys williams ...         1\n",
       "7       1797  french honour for director parker british film...         4\n",
       "8       2034  car giant hit by mercedes slump a slump in pro...         2\n",
       "9       1866  fockers fuel festive film chart comedy meet th...         4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBC_Data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea798add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a74d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [i for i in stopwords.words('english') if \"n't\" not in i and i not in ('not','no')]\n",
    "\n",
    "def process_text(text):    \n",
    "    text = word_tokenize(text) # tokenize words in text\n",
    "    text = [re.sub('[^A-Za-z0-9]+', '', word) for word in text] # this line substitutes any white space before the word by removing the space\n",
    "    text = [word.translate(str.maketrans('', '', string.punctuation)) for word in text]\n",
    "    text = [word.lower() for word in text if word.isalpha()] # lower each word in text\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text] # lemmatization of words, so when see persons an person, both are dealt as one word person\n",
    "    text = ' '.join(text) # join words into text again\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6fec877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom exboss launch defence lawyer defending former worldcom chief bernie ebbers battery fraud charge called company whistleblower first witness cynthia cooper worldcom exhead internal accounting alerted director irregular accounting practice u telecom giant warning led collapse firm following discovery accounting fraud mr ebbers pleaded not guilty charge fraud conspiracy prosecution lawyer argued mr ebbers orchestrated series accounting trick worldcom ordering employee hide expense inflate revenue meet wall street earnings estimate m cooper run consulting business told jury new york wednesday external auditor arthur andersen approved worldcom accounting early said andersen given green light procedure practice used worldcom mr ebber lawyer said unaware fraud arguing auditor not alert problem m cooper also said shareholder meeting mr ebbers often passed technical question company finance chief giving brief answer prosecution star witness former worldcom financial chief scott sullivan said mr ebbers ordered accounting adjustment firm telling hit book however m cooper said mr sullivan not mentioned anything uncomfortable worldcom accounting audit committee meeting mr ebbers could face jail sentence year convicted charge facing worldcom emerged bankruptcy protection known mci last week mci agreed buyout verizon communication deal valued'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = BBC_Data.Text[0]\n",
    "process_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "189ac795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363     ronaldo considering new contract manchester un...\n",
       "1243    jarvis sell tube stake spain share engineering...\n",
       "326     ore cost hit global steel firm share steel fir...\n",
       "807     uk firm face venezuelan land row venezuelan au...\n",
       "150     collins call chamber return world champion kim...\n",
       "1453    rock group korn guitarist quits guitarist u ro...\n",
       "1357    hague sixfigure earnings shown reward leaving ...\n",
       "366     lib dems bold election policy charles kennedy ...\n",
       "1213    benitez deflects blame dudek liverpool manager...\n",
       "819     new rule tackle sham wedding new rule marriage...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBC_Data['Text'] = BBC_Data['Text'].apply(process_text) # this line applies process_text function to Sentence in dataset\n",
    "BBC_Data['Text'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01d9f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_num_of_words(text):\n",
    "    return len(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b538f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1653"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBC_Data['Text'].apply(cal_num_of_words).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b28f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization & Padding \n",
      " blair reject iraq advice call tony blair rejected call publication advice legality iraq war amid growing call investigation prime minister told monthly press conference matter dealt attorney general earlier conservative mp michael mate joined call probe claim lord goldsmith statement parliament drawn number mr blair said statement fair summary lord goldsmith opinion lord goldsmith said say dealt time time time mr blair told monthly news conference downing street refused answer question issue saying dealt literally score time position not changed lord goldsmith denied leaned say word written government refuse publish advice legality war saying paper always kept confidential mr mate member common intelligence security committee part butler inquiry prewar intelligence told bbc friday general rule right not absolute rule said occasion advice published recently regarding prince charles marriage plan government could not pick choose use convention said mr mate added discovered two three occasion past law officer advice government published may one special occasion would public interest see advice attorney general gave prime minister argument rejected mr blair said firstly broken precedent secondly peter goldsmith made statement got absolutely nothing add book published week philippe sand qc member cherie blair matrix chamber say lord goldsmith warned tony blair march iraq war could illegal without second un resolution sanctioning military action short statement lord goldsmith position presented written parliamentary answer march crucial common vote military action mr sand book suggests actually written home office minister lord falconer downing street adviser baroness morgan former minister clare short resigned government iraq war said statement earlier shown cabinet discussed military action told bbc full advice attached according ministerial code view need house lord set special committee summon attorney get paper look exactly happened said conservative liberal democrat say want publication full legal advice given attorney general thursday lord goldsmith said statement not written number parliamentary answer march explained genuinely held independent view military action lawful existing security council resolution said\n",
      "After Tokenization & Padding \n",
      " [ 173 1700  509 1439  944 1360  914  413 1472  450  358 1701  168    3\n",
      " 3457  305  951 1017  944   58  171   54  173 2300 1279  475 2885 4289\n",
      " 1537  117   54  649 2886   20  477  402    1  345  299  712  986 2887\n",
      " 1701  168   30   53  471 1136  134 4290  871  503   91  259  173   47\n",
      "  676  571 1471   25  770  184 2088 1052    1  371  517  518   16   59\n",
      " 2640  471  289 1136  227 1471  167  554  173 1700    1  345    2  944\n",
      "   27 1360  914  413 1702 4291  281  617  503 1701  168 1121  187  288\n",
      " 2093    1]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 5000\n",
    "max_len=100\n",
    "\n",
    "def tokenize_pad_sequences(text):\n",
    "    '''\n",
    "    This function tokenize the input text into sequnences of intergers and then\n",
    "    pad each sequence to the same length\n",
    "    '''\n",
    "    # Text tokenization\n",
    "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    # Transforms text to a sequence of integers\n",
    "    X = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to the same length\n",
    "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "    # return sequences\n",
    "    return X, tokenizer\n",
    "\n",
    "print('Before Tokenization & Padding \\n', BBC_Data.loc[10, 'Text'])\n",
    "X, tokenizer = tokenize_pad_sequences(BBC_Data['Text'])\n",
    "print('After Tokenization & Padding \\n', X[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fcd15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bea6dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set -> (1117, 100) (1117, 5)\n",
      "Validation Set -> (373, 100) (373, 5)\n"
     ]
    }
   ],
   "source": [
    "y = pd.get_dummies(BBC_Data['Category'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "print('Train Set ->', X_train.shape, y_train.shape)\n",
    "print('Validation Set ->', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "777ed1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    ''' Function to calculate f1 score '''\n",
    "    \n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2a31cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.optimizers import SGD\n",
    "from keras import datasets\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import History\n",
    "\n",
    "from keras import losses\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_size = 64\n",
    "epochs=100\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "\n",
    "sgd = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "# Build model\n",
    "LSTM_model= Sequential()\n",
    "LSTM_model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
    "LSTM_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "LSTM_model.add(MaxPooling1D(pool_size=2))\n",
    "LSTM_model.add(Bidirectional(LSTM(64)))\n",
    "#LSTM_model.add(Dropout(0.4))\n",
    "LSTM_model.add(Dense(5, activation='softmax'))\n",
    "LSTM_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4cbeb5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.keras.utils.plot_model(LSTM_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ff81066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 100, 64)           320000    \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 100, 32)           6176      \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 50, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 128)              49664     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 376,485\n",
      "Trainable params: 376,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 4s 85ms/step - loss: 1.6042 - accuracy: 0.2265 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 1.6148 - val_accuracy: 0.2118 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.6049 - accuracy: 0.2355 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 1.6049 - val_accuracy: 0.2520 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 1.6006 - accuracy: 0.2534 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 1.6126 - val_accuracy: 0.2118 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 1.5970 - accuracy: 0.2739 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 1.6040 - val_accuracy: 0.2118 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 1.5847 - accuracy: 0.2659 - precision_17: 0.0000e+00 - recall_17: 0.0000e+00 - val_loss: 1.5778 - val_accuracy: 0.2520 - val_precision_17: 0.0000e+00 - val_recall_17: 0.0000e+00\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.5703 - accuracy: 0.2534 - precision_17: 0.5676 - recall_17: 0.0188 - val_loss: 1.5689 - val_accuracy: 0.3164 - val_precision_17: 0.6000 - val_recall_17: 0.0402\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.5618 - accuracy: 0.2480 - precision_17: 0.5733 - recall_17: 0.0385 - val_loss: 1.5546 - val_accuracy: 0.2547 - val_precision_17: 0.5429 - val_recall_17: 0.0509\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.5477 - accuracy: 0.3026 - precision_17: 0.5873 - recall_17: 0.0331 - val_loss: 1.5511 - val_accuracy: 0.2279 - val_precision_17: 0.4565 - val_recall_17: 0.0563\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 1.5276 - accuracy: 0.3187 - precision_17: 0.6250 - recall_17: 0.0582 - val_loss: 1.5189 - val_accuracy: 0.4987 - val_precision_17: 0.5294 - val_recall_17: 0.0483\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.4406 - accuracy: 0.4503 - precision_17: 0.5841 - recall_17: 0.0591 - val_loss: 1.9799 - val_accuracy: 0.2976 - val_precision_17: 0.6875 - val_recall_17: 0.0295\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 1.7045 - accuracy: 0.2587 - precision_17: 0.5833 - recall_17: 0.0313 - val_loss: 1.5479 - val_accuracy: 0.2493 - val_precision_17: 0.5152 - val_recall_17: 0.0456\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.5323 - accuracy: 0.3277 - precision_17: 0.5862 - recall_17: 0.0457 - val_loss: 1.5308 - val_accuracy: 0.2547 - val_precision_17: 0.5667 - val_recall_17: 0.0456\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.4909 - accuracy: 0.3312 - precision_17: 0.6429 - recall_17: 0.0403 - val_loss: 1.4884 - val_accuracy: 0.2735 - val_precision_17: 0.5312 - val_recall_17: 0.0456\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 1.4722 - accuracy: 0.4270 - precision_17: 0.4783 - recall_17: 0.0689 - val_loss: 1.4552 - val_accuracy: 0.3646 - val_precision_17: 0.5641 - val_recall_17: 0.0590\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.3741 - accuracy: 0.4324 - precision_17: 0.8214 - recall_17: 0.0824 - val_loss: 1.2603 - val_accuracy: 0.4531 - val_precision_17: 0.9062 - val_recall_17: 0.0777\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 1.0549 - accuracy: 0.5577 - precision_17: 0.8028 - recall_17: 0.2551 - val_loss: 0.9721 - val_accuracy: 0.5684 - val_precision_17: 0.8588 - val_recall_17: 0.3914\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.6984 - accuracy: 0.7386 - precision_17: 0.8625 - recall_17: 0.5560 - val_loss: 0.8489 - val_accuracy: 0.6166 - val_precision_17: 0.7489 - val_recall_17: 0.4558\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.8271 - accuracy: 0.6392 - precision_17: 0.7289 - recall_17: 0.5488 - val_loss: 1.1560 - val_accuracy: 0.5496 - val_precision_17: 0.6027 - val_recall_17: 0.3539\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.6809 - accuracy: 0.7466 - precision_17: 0.8437 - recall_17: 0.5944 - val_loss: 0.6589 - val_accuracy: 0.7560 - val_precision_17: 0.8577 - val_recall_17: 0.6461\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.3163 - accuracy: 0.9006 - precision_17: 0.9369 - recall_17: 0.8639 - val_loss: 0.6099 - val_accuracy: 0.7989 - val_precision_17: 0.8212 - val_recall_17: 0.7882\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.2573 - accuracy: 0.9078 - precision_17: 0.9175 - recall_17: 0.8962 - val_loss: 0.5160 - val_accuracy: 0.8284 - val_precision_17: 0.8361 - val_recall_17: 0.8070\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.1960 - accuracy: 0.9364 - precision_17: 0.9494 - recall_17: 0.9239 - val_loss: 0.5260 - val_accuracy: 0.8204 - val_precision_17: 0.8238 - val_recall_17: 0.8150\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.3106 - accuracy: 0.8854 - precision_17: 0.8954 - recall_17: 0.8738 - val_loss: 0.5138 - val_accuracy: 0.8204 - val_precision_17: 0.8251 - val_recall_17: 0.8097\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0843 - accuracy: 0.9848 - precision_17: 0.9856 - recall_17: 0.9812 - val_loss: 0.5375 - val_accuracy: 0.8338 - val_precision_17: 0.8462 - val_recall_17: 0.8257\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0380 - accuracy: 0.9946 - precision_17: 0.9946 - recall_17: 0.9937 - val_loss: 0.4781 - val_accuracy: 0.8660 - val_precision_17: 0.8767 - val_recall_17: 0.8579\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0320 - accuracy: 0.9937 - precision_17: 0.9946 - recall_17: 0.9928 - val_loss: 0.3858 - val_accuracy: 0.8928 - val_precision_17: 0.9044 - val_recall_17: 0.8874\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0174 - accuracy: 0.9991 - precision_17: 0.9991 - recall_17: 0.9991 - val_loss: 0.4212 - val_accuracy: 0.8847 - val_precision_17: 0.8841 - val_recall_17: 0.8794\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0161 - accuracy: 0.9991 - precision_17: 0.9991 - recall_17: 0.9991 - val_loss: 0.4813 - val_accuracy: 0.8820 - val_precision_17: 0.8913 - val_recall_17: 0.8794\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0092 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4080 - val_accuracy: 0.8794 - val_precision_17: 0.8862 - val_recall_17: 0.8767\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 38ms/step - loss: 0.2365 - accuracy: 0.9096 - precision_17: 0.9167 - recall_17: 0.9069 - val_loss: 0.7273 - val_accuracy: 0.7346 - val_precision_17: 0.7679 - val_recall_17: 0.7185\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.1117 - accuracy: 0.9812 - precision_17: 0.9828 - recall_17: 0.9714 - val_loss: 0.4791 - val_accuracy: 0.8418 - val_precision_17: 0.8532 - val_recall_17: 0.8257\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0248 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4515 - val_accuracy: 0.8713 - val_precision_17: 0.8812 - val_recall_17: 0.8552\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0123 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4427 - val_accuracy: 0.8740 - val_precision_17: 0.8871 - val_recall_17: 0.8633\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0083 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4547 - val_accuracy: 0.8713 - val_precision_17: 0.8753 - val_recall_17: 0.8660\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0062 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4576 - val_accuracy: 0.8794 - val_precision_17: 0.8859 - val_recall_17: 0.8740\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0050 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4636 - val_accuracy: 0.8740 - val_precision_17: 0.8760 - val_recall_17: 0.8713\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0043 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4586 - val_accuracy: 0.8820 - val_precision_17: 0.8841 - val_recall_17: 0.8794\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0036 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4661 - val_accuracy: 0.8820 - val_precision_17: 0.8844 - val_recall_17: 0.8820\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0031 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4737 - val_accuracy: 0.8794 - val_precision_17: 0.8790 - val_recall_17: 0.8767\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0028 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4765 - val_accuracy: 0.8767 - val_precision_17: 0.8787 - val_recall_17: 0.8740\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0025 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4807 - val_accuracy: 0.8767 - val_precision_17: 0.8787 - val_recall_17: 0.8740\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0023 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4790 - val_accuracy: 0.8794 - val_precision_17: 0.8814 - val_recall_17: 0.8767\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0021 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4853 - val_accuracy: 0.8794 - val_precision_17: 0.8814 - val_recall_17: 0.8767\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0019 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4876 - val_accuracy: 0.8820 - val_precision_17: 0.8865 - val_recall_17: 0.8794\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0018 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4920 - val_accuracy: 0.8794 - val_precision_17: 0.8838 - val_recall_17: 0.8767\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0017 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4949 - val_accuracy: 0.8794 - val_precision_17: 0.8814 - val_recall_17: 0.8767\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0016 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.4960 - val_accuracy: 0.8820 - val_precision_17: 0.8841 - val_recall_17: 0.8794\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0015 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5017 - val_accuracy: 0.8794 - val_precision_17: 0.8814 - val_recall_17: 0.8767\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0014 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5026 - val_accuracy: 0.8820 - val_precision_17: 0.8841 - val_recall_17: 0.8794\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0013 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5064 - val_accuracy: 0.8794 - val_precision_17: 0.8841 - val_recall_17: 0.8794\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0013 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5073 - val_accuracy: 0.8820 - val_precision_17: 0.8868 - val_recall_17: 0.8820\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0012 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5107 - val_accuracy: 0.8820 - val_precision_17: 0.8868 - val_recall_17: 0.8820\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0012 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5124 - val_accuracy: 0.8820 - val_precision_17: 0.8844 - val_recall_17: 0.8820\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0011 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5146 - val_accuracy: 0.8820 - val_precision_17: 0.8844 - val_recall_17: 0.8820\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0011 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5182 - val_accuracy: 0.8820 - val_precision_17: 0.8844 - val_recall_17: 0.8820\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.0010 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5190 - val_accuracy: 0.8820 - val_precision_17: 0.8844 - val_recall_17: 0.8820\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0010 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5222 - val_accuracy: 0.8820 - val_precision_17: 0.8868 - val_recall_17: 0.8820\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 9.7446e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5246 - val_accuracy: 0.8820 - val_precision_17: 0.8844 - val_recall_17: 0.8820\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 9.4230e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5247 - val_accuracy: 0.8847 - val_precision_17: 0.8868 - val_recall_17: 0.8820\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 9.1332e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5286 - val_accuracy: 0.8820 - val_precision_17: 0.8844 - val_recall_17: 0.8820\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 8.8634e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5299 - val_accuracy: 0.8847 - val_precision_17: 0.8892 - val_recall_17: 0.8820\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 8.6246e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5326 - val_accuracy: 0.8847 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 8.3827e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5338 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 37ms/step - loss: 8.1734e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5358 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 7.9695e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5359 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 7.7754e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5391 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 7.5821e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5412 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 7.4122e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5419 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 7.2510e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5431 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 7.0934e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5447 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 6.9500e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5446 - val_accuracy: 0.8901 - val_precision_17: 0.8895 - val_recall_17: 0.8847\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 6.8036e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5471 - val_accuracy: 0.8901 - val_precision_17: 0.8895 - val_recall_17: 0.8847\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 6.6745e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5490 - val_accuracy: 0.8901 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 6.5461e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5488 - val_accuracy: 0.8901 - val_precision_17: 0.8898 - val_recall_17: 0.8874\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 6.4137e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5502 - val_accuracy: 0.8928 - val_precision_17: 0.8925 - val_recall_17: 0.8901\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 6.2960e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5510 - val_accuracy: 0.8928 - val_precision_17: 0.8925 - val_recall_17: 0.8901\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 6.1909e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5544 - val_accuracy: 0.8928 - val_precision_17: 0.8925 - val_recall_17: 0.8901\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 6.0785e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5554 - val_accuracy: 0.8928 - val_precision_17: 0.8925 - val_recall_17: 0.8901\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.9725e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5546 - val_accuracy: 0.8901 - val_precision_17: 0.8949 - val_recall_17: 0.8901\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.8799e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5565 - val_accuracy: 0.8901 - val_precision_17: 0.8949 - val_recall_17: 0.8901\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.7840e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5569 - val_accuracy: 0.8901 - val_precision_17: 0.8949 - val_recall_17: 0.8901\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.6925e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5587 - val_accuracy: 0.8901 - val_precision_17: 0.8946 - val_recall_17: 0.8874\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.6083e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5596 - val_accuracy: 0.8874 - val_precision_17: 0.8946 - val_recall_17: 0.8874\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.5243e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5610 - val_accuracy: 0.8874 - val_precision_17: 0.8946 - val_recall_17: 0.8874\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.4427e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5616 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.3618e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5626 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.2913e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5643 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.2166e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5656 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.1442e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5657 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 5.0813e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5655 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 5.0094e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5688 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.9454e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5684 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.8853e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5706 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.8251e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5707 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.7650e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5697 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.7099e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5709 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.6549e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5719 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 37ms/step - loss: 4.6011e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5720 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 4.5500e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5732 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 4.4991e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.5740 - val_accuracy: 0.8874 - val_precision_17: 0.8922 - val_recall_17: 0.8874\n"
     ]
    }
   ],
   "source": [
    "print(LSTM_model.summary())\n",
    "# Compile model\n",
    "LSTM_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Train model\n",
    "\n",
    "batch_size = 64\n",
    "history = LSTM_model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                         batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4c45f49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 2.3951e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6125 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3868e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6125 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3775e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6129 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3689e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3603e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6135 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.3518e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6138 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3436e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6139 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 2.3353e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6143 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.3270e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6145 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.3189e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6148 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.3111e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6152 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.3030e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6151 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2950e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6153 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2871e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6158 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2795e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6159 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2716e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6161 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2643e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6163 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2564e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6164 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2493e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6166 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2421e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6168 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2347e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2277e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6172 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2202e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6176 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2137e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6177 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.2062e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1994e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6183 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1927e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6185 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1857e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6187 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1793e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6186 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1723e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6190 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1656e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6193 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1592e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6197 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1529e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6199 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1464e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6201 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1401e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6201 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1338e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6203 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1277e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6203 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1217e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6209 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1155e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6210 - val_accuracy: 0.8874 - val_precision_17: 0.8874 - val_recall_17: 0.8874\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1095e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6213 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.1033e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6214 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0973e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6216 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0914e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6218 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0855e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6220 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0795e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6220 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0738e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6223 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0681e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6223 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0623e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6224 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0567e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6226 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 2.0512e-04 - accuracy: 1.0000 - precision_17: 1.0000 - recall_17: 1.0000 - val_loss: 0.6229 - val_accuracy: 0.8874 - val_precision_17: 0.8871 - val_recall_17: 0.8847\n"
     ]
    }
   ],
   "source": [
    "history = LSTM_model.fit(X_train, y_train,\n",
    "                      validation_data=(X_val, y_val),\n",
    "                      batch_size=batch_size, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2e756971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 6ms/step\n",
      "Accuracy : 0.8874\n",
      "Precision: 0.8905\n",
      "Recall   : 0.8874\n",
      "F1 Score : 0.8881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluate model on the test set\n",
    "y_pred_probs = LSTM_model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "accuracy = accuracy_score(y_val.values.argmax(axis=1), y_pred)\n",
    "precision = precision_score(y_val.values.argmax(axis=1), y_pred, average='weighted')\n",
    "recall = recall_score(y_val.values.argmax(axis=1), y_pred, average='weighted')\n",
    "f1 = f1_score(y_val.values.argmax(axis=1), y_pred, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print('Accuracy : {:.4f}'.format(accuracy))\n",
    "print('Precision: {:.4f}'.format(precision))\n",
    "print('Recall   : {:.4f}'.format(recall))\n",
    "print('F1 Score : {:.4f}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7796de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(BBC_Data['Text'],BBC_Data['Category'],\n",
    "                                                  stratify=BBC_Data['Category'],test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "62ecdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(list(zip(X_train, y_train)), columns=['Text', 'Category'])\n",
    "df_val = pd.DataFrame(list(zip(X_val, y_val)), columns=['Text', 'Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "89cd6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(df_train['Category'])\n",
    "y_val = tf.keras.utils.to_categorical(df_val['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5f0159e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 6)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9c720676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       tory urge change top tory delegate gathering e...\n",
       "1       kennedy call iraq exit plan tony blair set pro...\n",
       "2       xbox may unveiled summer detail next generatio...\n",
       "3       mp tout lord replacement plan group mp tried r...\n",
       "4       indecency fine viacom medium giant viacom paid...\n",
       "                              ...                        \n",
       "1187    jowell reject la vega jibe secretary state cul...\n",
       "1188    highdefinition dvd first humble home video dvd...\n",
       "1189    labour eu propaganda taxpayer subsidised propa...\n",
       "1190    baghdad blogger big screen film based internet...\n",
       "1191    capriati miss melbourne jennifer capriati beco...\n",
       "Name: Text, Length: 1192, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "66eef88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model BERT\n",
    "from transformers import AutoTokenizer, TFBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizing\n",
    "max_len = 100 # from histogram length words\n",
    "x_train = tokenizer(text=df_train['Text'].tolist(),\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=max_len,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='tf',\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=True,\n",
    "                    verbose=True)\n",
    "x_test = tokenizer(text=df_val['Text'].tolist(),\n",
    "                   add_special_tokens=True,\n",
    "                   max_length=max_len,\n",
    "                   truncation=True,\n",
    "                   padding='max_length',\n",
    "                   return_tensors='tf',\n",
    "                   return_token_type_ids=False,\n",
    "                   return_attention_mask=True,\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b0f13f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 100,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 768)         0           ['tf_bert_model_2[0][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 128)          98432       ['global_max_pooling1d_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_113 (Dropout)          (None, 128)          0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 32)           4128        ['dropout_113[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 6)            198         ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,584,998\n",
      "Trainable params: 109,584,998\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model fine tuning bert\n",
    "input_ids = tf.keras.layers.Input(\n",
    "    shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = tf.keras.layers.Input(\n",
    "    shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=input_mask)[0]\n",
    "\n",
    "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = tf.keras.layers.Dense(32, activation='relu')(out)\n",
    "\n",
    "y = tf.keras.layers.Dense(6, activation='softmax')(out)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "\n",
    "model.layers[2].trainable = True\n",
    "\n",
    "# set the decay schedule\n",
    "decay_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=5e-05,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.96)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=decay_schedule,\n",
    "    epsilon=1e-08,\n",
    "    clipnorm=1.0)\n",
    "\n",
    "# Set loss and metrics\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "metric = tf.keras.metrics.CategoricalAccuracy('balanced_accuracy')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metric)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6f2ee5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "38/38 [==============================] - 512s 13s/step - loss: 1.0754 - balanced_accuracy: 0.5956 - val_loss: 0.4718 - val_balanced_accuracy: 0.9027\n",
      "Epoch 2/3\n",
      "38/38 [==============================] - 487s 13s/step - loss: 0.3305 - balanced_accuracy: 0.9346 - val_loss: 0.1684 - val_balanced_accuracy: 0.9832\n",
      "Epoch 3/3\n",
      "38/38 [==============================] - 487s 13s/step - loss: 0.1006 - balanced_accuracy: 0.9824 - val_loss: 0.1012 - val_balanced_accuracy: 0.9866\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "history = model.fit(\n",
    "    x={'input_ids': x_train['input_ids'],\n",
    "        'attention_mask': x_train['attention_mask']},\n",
    "    y=y_train,\n",
    "    validation_data=(\n",
    "        {'input_ids': x_test['input_ids'], 'attention_mask': x_test['attention_mask']}, y_val),\n",
    "    epochs=3,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8ab83884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 40s 4s/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        sport       0.97      1.00      0.99        69\n",
      "     business       0.97      1.00      0.99        67\n",
      "     politics       1.00      0.98      0.99        55\n",
      "entertainment       1.00      1.00      1.00        55\n",
      "         tech       1.00      0.94      0.97        52\n",
      "\n",
      "     accuracy                           0.99       298\n",
      "    macro avg       0.99      0.98      0.99       298\n",
      " weighted avg       0.99      0.99      0.99       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "predicted_raw = model.predict(\n",
    "    {'input_ids': x_test['input_ids'], 'attention_mask': x_test['attention_mask']})\n",
    "\n",
    "y_predicted = np.argmax(predicted_raw, axis=1)\n",
    "y_true = df_val['Category']\n",
    "\n",
    "print(classification_report(y_true, y_predicted, target_names=['sport','business', 'politics', 'entertainment', 'tech']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6cbd13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_Data_Test = pd.read_csv('BBC News Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "68c1e13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018</td>\n",
       "      <td>qpr keeper day heads for preston queens park r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319</td>\n",
       "      <td>software watching while you work software that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1138</td>\n",
       "      <td>d arcy injury adds to ireland woe gordon d arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459</td>\n",
       "      <td>india s reliance family feud heats up the ongo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1923</td>\n",
       "      <td>eu to probe alitalia  state aid  the european ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>373</td>\n",
       "      <td>u2 to play at grammy awards show irish rock ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>1704</td>\n",
       "      <td>sport betting rules in spotlight a group of mp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>206</td>\n",
       "      <td>alfa romeos  to get gm engines  fiat is to sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>471</td>\n",
       "      <td>citizenship event for 18s touted citizenship c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>735 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ArticleId                                               Text\n",
       "0         1018  qpr keeper day heads for preston queens park r...\n",
       "1         1319  software watching while you work software that...\n",
       "2         1138  d arcy injury adds to ireland woe gordon d arc...\n",
       "3          459  india s reliance family feud heats up the ongo...\n",
       "4         1020  boro suffer morrison injury blow middlesbrough...\n",
       "..         ...                                                ...\n",
       "730       1923  eu to probe alitalia  state aid  the european ...\n",
       "731        373  u2 to play at grammy awards show irish rock ba...\n",
       "732       1704  sport betting rules in spotlight a group of mp...\n",
       "733        206  alfa romeos  to get gm engines  fiat is to sto...\n",
       "734        471  citizenship event for 18s touted citizenship c...\n",
       "\n",
       "[735 rows x 2 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBC_Data_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5c9a11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer(text=BBC_Data_Test['Text'].tolist(),\n",
    "                   add_special_tokens=True,\n",
    "                   max_length=max_len,\n",
    "                   truncation=True,\n",
    "                   padding='max_length',\n",
    "                   return_tensors='tf',\n",
    "                   return_token_type_ids=False,\n",
    "                   return_attention_mask=True,\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cc6ca4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 96s 4s/step\n"
     ]
    }
   ],
   "source": [
    "predicted_raw = model.predict(\n",
    "    {'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']})\n",
    "\n",
    "Y_predicted = np.argmax(predicted_raw, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "64b0cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicto_inv = {1:'sport',2:'business',3:'politics',4:'entertainment',5:'tech'}\n",
    "\n",
    "BBC_Data_Test['Predicted Category'] = np.vectorize(dicto_inv.get)(Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "45dd6019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Predicted Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018</td>\n",
       "      <td>qpr keeper day heads for preston queens park r...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1319</td>\n",
       "      <td>software watching while you work software that...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1138</td>\n",
       "      <td>d arcy injury adds to ireland woe gordon d arc...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459</td>\n",
       "      <td>india s reliance family feud heats up the ongo...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1923</td>\n",
       "      <td>eu to probe alitalia  state aid  the european ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>373</td>\n",
       "      <td>u2 to play at grammy awards show irish rock ba...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>1704</td>\n",
       "      <td>sport betting rules in spotlight a group of mp...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>206</td>\n",
       "      <td>alfa romeos  to get gm engines  fiat is to sto...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>471</td>\n",
       "      <td>citizenship event for 18s touted citizenship c...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>735 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ArticleId                                               Text  \\\n",
       "0         1018  qpr keeper day heads for preston queens park r...   \n",
       "1         1319  software watching while you work software that...   \n",
       "2         1138  d arcy injury adds to ireland woe gordon d arc...   \n",
       "3          459  india s reliance family feud heats up the ongo...   \n",
       "4         1020  boro suffer morrison injury blow middlesbrough...   \n",
       "..         ...                                                ...   \n",
       "730       1923  eu to probe alitalia  state aid  the european ...   \n",
       "731        373  u2 to play at grammy awards show irish rock ba...   \n",
       "732       1704  sport betting rules in spotlight a group of mp...   \n",
       "733        206  alfa romeos  to get gm engines  fiat is to sto...   \n",
       "734        471  citizenship event for 18s touted citizenship c...   \n",
       "\n",
       "    Predicted Category  \n",
       "0                sport  \n",
       "1                 tech  \n",
       "2                sport  \n",
       "3             business  \n",
       "4                sport  \n",
       "..                 ...  \n",
       "730           business  \n",
       "731      entertainment  \n",
       "732           politics  \n",
       "733           business  \n",
       "734           politics  \n",
       "\n",
       "[735 rows x 3 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBC_Data_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4feef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
